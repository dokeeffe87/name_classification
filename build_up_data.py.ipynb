{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up data set with relevant metrics for string matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jellyfish\n",
    "import string\n",
    "import collections\n",
    "import re\n",
    "import nltk.corpus\n",
    "import nltk.tokenize.punkt\n",
    "import nltk.stem.snowball\n",
    "import ngram\n",
    "\n",
    "from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance as normed_dm_dist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options for convenience\n",
    "pd.options.display.max_columns=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "df = pd.read_csv('matched_strings.csv', header=0, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fuzzy_match_show_name</th>\n",
       "      <th>fuzzy_match_show_name_2</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#THROWBACKTHURSDAY</td>\n",
       "      <td>THROWBACKTHURSDAY VIDEOS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 THINGS HATE ABOUT YO</td>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>WILD THINGS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 GREATEST KIDS STARS</td>\n",
       "      <td>THE GREATEST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fuzzy_match_show_name   fuzzy_match_show_name_2  is_correct\n",
       "0        #THROWBACKTHURSDAY  THROWBACKTHURSDAY VIDEOS           1\n",
       "1   10 THINGS HATE ABOUT YO  10 THINGS HATE ABOUT YOU           1\n",
       "2  10 THINGS HATE ABOUT YOU  10 THINGS HATE ABOUT YOU           1\n",
       "3  10 THINGS HATE ABOUT YOU               WILD THINGS           0\n",
       "4   100 GREATEST KIDS STARS              THE GREATEST           0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building out features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Helper function for making tokens out of string str_1 and str_2\n",
    "    \"\"\"\n",
    "    tokens_a = [token.lower().strip(string.punctuation) for token in tokenizer(str_1) if \n",
    "               token.lower().strip(string.punctuation)]\n",
    "    tokens_b = [token.lower().strip(string.punctuation) for token in tokenizer(str_2) if \n",
    "               token.lower().strip(string.punctuation)]\n",
    "        \n",
    "    return tokens_a, tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stems(tokens_a, tokens_b):\n",
    "    \"\"\"\n",
    "    Makes stems out of tokens tokens_a and tokens_b\n",
    "    \"\"\"\n",
    "    stem_a = [stemmer.stem(token) for token in tokens_a]\n",
    "    stem_b = [stemmer.stem(token) for token in tokens_b]\n",
    "    \n",
    "    return stem_a, stem_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Implements Jaccard similarity between input strings str_1, str_2\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b = make_tokens(str_1, str_2)\n",
    "    stem_a, stem_b = make_stems(tokens_a, tokens_b)\n",
    "        \n",
    "    return len(set(stem_a).intersection(set(stem_b))) / float(len(set(stem_a).union(set(stem_b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonetic_matching_str(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Computes phonetic distance between input strings\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b = make_tokens(str_1, str_2)\n",
    "    stem_a, stem_b = make_stems(tokens_a, tokens_b)\n",
    "    \n",
    "    phonetic_distance = jellyfish.jaro_distance(unicode(jellyfish.metaphone(unicode(' '.join(stem_a)))), \n",
    "                            unicode(jellyfish.metaphone(unicode(' '.join(stem_b)))))\n",
    "    return phonetic_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_matching_str(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Implements trigram distance between strings str_1 and str_2\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b = make_tokens(str_1, str_2)\n",
    "    \n",
    "    joined_a = ' '.join(tokens_a)\n",
    "    joined_b = ' '.join(tokens_b)\n",
    "    # TODO: Generalize to other N-grams\n",
    "    trigram_compare = ngram.NGram.compare(joined_a, joined_b, N=3)\n",
    "    \n",
    "    return trigram_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DL_distance(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Computes DL distance between strings str_1 and str_2\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b = make_tokens(str_1, str_2)\n",
    "    stem_a, stem_b = make_stems(tokens_a, tokens_b)\n",
    "    \n",
    "    DL_sim = 1 - normed_dm_dist(' '.join(stem_a), ' '.join(stem_b))\n",
    "    \n",
    "    return DL_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_distance(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Returns the conventional fuzzy match distance between two strings.\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b = make_tokens(str_1, str_2)\n",
    "    stem_a, stem_b = make_stems(tokens_a, tokens_b)\n",
    "    \n",
    "    fuzzy_dist = fuzz.ratio(' '.join(stem_a), ' '.join(stem_b)) / 100\n",
    "    return fuzzy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(str_):\n",
    "    \"\"\"\n",
    "    Function to define tokenizer\n",
    "    \"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(str_)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lemmatizer and stemmer\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stemmer = nltk.snowball.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build out the features on the original input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index=str, columns={'fuzzy_match_show_name': 'str_1', \n",
    "                              'fuzzy_match_show_name_2': 'str_2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fuzzy_similarity'] = df.apply(lambda row: fuzzy_distance(str_1=row['str_1'], \n",
    "                                                             str_2=row['str_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DL_similarity'] = df.apply(lambda row: DL_distance(str_1=row['str_1'], \n",
    "                                                       str_2=row['str_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jaccard_similarity'] = df.apply(lambda row: jaccard_similarity(str_1=row['str_1'],\n",
    "                                                                  str_2=row['str_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ngram_compare'] = df.apply(lambda row: n_gram_matching_str(str_1=row['str_1'], \n",
    "                                                               str_2=row['str_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['phonetic_distance'] = df.apply(lambda row: phonetic_matching_str(str_1=row['str_1'],\n",
    "                                                                    str_2=row['str_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_1</th>\n",
       "      <th>str_2</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>fuzzy_similarity</th>\n",
       "      <th>DL_similarity</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>ngram_compare</th>\n",
       "      <th>phonetic_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#THROWBACKTHURSDAY</td>\n",
       "      <td>THROWBACKTHURSDAY VIDEOS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 THINGS HATE ABOUT YO</td>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 THINGS HATE ABOUT YOU</td>\n",
       "      <td>WILD THINGS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.576984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 GREATEST KIDS STARS</td>\n",
       "      <td>THE GREATEST</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      str_1                     str_2  is_correct  \\\n",
       "0        #THROWBACKTHURSDAY  THROWBACKTHURSDAY VIDEOS           1   \n",
       "1   10 THINGS HATE ABOUT YO  10 THINGS HATE ABOUT YOU           1   \n",
       "2  10 THINGS HATE ABOUT YOU  10 THINGS HATE ABOUT YOU           1   \n",
       "3  10 THINGS HATE ABOUT YOU               WILD THINGS           0   \n",
       "4   100 GREATEST KIDS STARS              THE GREATEST           0   \n",
       "\n",
       "   fuzzy_similarity  DL_similarity  jaccard_similarity  ngram_compare  \\\n",
       "0              0.85       0.739130            0.500000       0.607143   \n",
       "1              0.98       0.956522            0.666667       0.821429   \n",
       "2              1.00       1.000000            1.000000       1.000000   \n",
       "3              0.36       0.173913            0.166667       0.147059   \n",
       "4              0.55       0.428571            0.200000       0.218750   \n",
       "\n",
       "   phonetic_distance  \n",
       "0           0.909091  \n",
       "1           1.000000  \n",
       "2           1.000000  \n",
       "3           0.576984  \n",
       "4           0.619048  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the whole data set that we need for classification.\n",
    "df.to_csv('full_matching_data_set.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:name_matching]",
   "language": "python",
   "name": "conda-env-name_matching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
